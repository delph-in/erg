include "common".
include "mrs".

;start-symbols := $root_strict.
;start-symbols := $root_strict $root_informal.
start-symbols := $root_strict $root_frag $root_informal $root_inffrag.
;start-symbols := $root_strict $root_frag $root_informal $root_inffrag $root_robust.
;start-symbols := $root_strict $root_frag.
;start-symbols := $root_strict $root_informal $root_robust.

;punctuation-characters := "<>{}+*`".
punctuation-characters := "".
case-sensitive.               ; be case sensitive in string and yy tokenizer
;trivial-tokenizer.            ; for Lingo-style tokenizer "string"

;;;
;;; following are a number of settings for the new (as of late 2008) token
;;; mapping and lexical filtering support in PET.
;;;

;;;
;;; first, the general format of chart mapping rules, much like MRS transfer.
;;;
chart-mapping-context-path  := "+CONTEXT".
chart-mapping-input-path    := "+INPUT".
chart-mapping-output-path   := "+OUTPUT".
chart-mapping-position-path := "+POSITION".
;;;
;;; in lexical instatiation, the list of tokens activating a lexical entry (be
;;; it native or generic) are unified into the lexical entry under this path.
;;;
lexicon-tokens-path := "TOKENS".
;;;
;;; furthermore, for the various input formats, we need to declare how parts of
;;; input descriptions correspond to the grammar-internal feature geometry; in
;;; the YY input format, for example, token feature structures (aka input items
;;; PET-internally) are created from various parts of the token description.
;;; 
token-form-path     := "+FORM".       ; [required] string for lexical lookup
token-ids-path      := "+IDS".        ; [optional] list of external ids
token-cfrom-path    := "+CFROM".      ; [optional] character start position
token-cto-path      := "+CTO".        ; [optional] chracter end position
token-postags-path  := "+TNT.+TAGS".  ; [optional] list of POS tags
token-posprobs-path := "+TNT.+PRBS".  ; [optional] list of POS probabilities
