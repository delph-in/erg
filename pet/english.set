;;;
;;; This file is part of the English Level 2 Language model.  It should be
;;; kept in the same directory as the accompanying files `english.grm' and 
;;; `l2global.set'.  Do not read any further; if you feel you have to, do not,
;;; under no circumstances, whatsover, change any of the following.
;;;

encoding := iso-8859-1.

include "global".
include "k2y".

;;
;; type of a valid parse --- used only for CSLI test suite
;;
start-symbols := $root_strict.

;;
;; word-breaking characters in tokenization --- treated as whitespace
;;
;;punctuation-characters := "?!.:;,()<>{}[]+*-`_\"".
punctuation-characters := ",()<>{}[]+*`_\"".

;;
;; weighted start symbols: pairs of root type plus threshold; only once the
;; top score on the agenda drops below the threshold, the associated root will
;; be allowed to license results.  when the search space is exhausted, the
;; agenda score drops to 0, eventually.
;;
;; when set, `weighted-start-symbols' takes precedence over `start-symbols'.
;;
#|
weighted-start-symbols := 
  $root_strict 1000 
  $root_lex 1000
  $root_gap 800
  $root_conj 200
  $root_phr 50
  $root_subord 50
.
|#

;;
;; suppress results of regular morphology rules if irregular form exists
;;
irregular-forms-only.

lex-entries-can-fail.

;;
;; names of attributes not to pass from daughter to mother in parsing
;;
deleted-daughters := ARGS HEAD-DTR NON-HEAD-DTR LCONJ-DTR RCONJ-DTR DTR1 DTR2
                     MOD-DTR NONMOD-DTR.

;;
;; names of attributes to filter for packing parser
;;
packing-restrictor := RELS HCONS.

;;
;; path into the MRS semantics
;;
mrs-path := "SYNSEM.LOCAL.CONT".

;;
;; path to LABEL for relation-to-word mapping, absolute and within a relation
;;
label-path := "SYNSEM.LOCAL.KEYS.KEY.LABEL".
label-path-tail := "LABEL".

;;
;; name of type containing quick check structure
;;
qc-structure := $qc_paths.

;;;
;;; generic lexical entries for unknown words: for each unknown word in the
;;; input all generic entries are postulated.  those that require a certain
;;; suffix (`generic-le-suffixes') only fire if the input form has the suffix.
;;; scoring for generic items is based on the default priority (typically 
;;; fairly low) and (optionally) adjusted on the basis of POS information that
;;; may be available for the unknown word.  if the input word has one more more
;;; POS tags associated to it, these are looked up in the `posmapping' table:
;;; this table is a list of triples (tag, score, gle) where `gle' is the name
;;; of one of the generic items in `generic-les'.  for each generic item, the
;;; score is adjusted to the first match of one of the tags associated with the
;;; unknown word in the mapping table.
;;;

generic-lexentry-status-values := generic-lex-entry.

;;;
;;; default initial priority assigned to generic lexical entries; this may be
;;; re-adjusted by later steps in unknown word handling.  generic entries that
;;; end up with a zero priority are dropped before parsing.
;;;
default-gen-le-priority := 0.

;;;
;;; some generic lexical entries require inflectional marking.  this mechanism
;;; is a filter on which generic entries proposed by other means can survive:
;;; generic entries listed here will only be postulated if the required suffix 
;;; can be matched against the input token.
;;;
;;; when using only the generic entries licensed by a POS tag (i.e. a default
;;; priority of 0 and a non-empty `posmapping'), the suffix filter really does
;;; not make a lot of sense anymore.
;;;
generic-le-suffixes := 
#|
  $generic_trans_verb_pres3sg "S" 
  $generic_trans_verb_past "ED" 
  $generic_trans_verb_psp "ED" 
  $generic_trans_verb_prp "ING" 
  $generic_pl_noun "S"
|#
.

posmapping := 
  UpperAndLowerCase 800 $genericname
  UpperAndLowerCaseInitial 200 $genericname
  JJ 100 $generic_adj
  JJR 100 $generic_adj_compar
  JJS 100 $generic_adj_superl
  NN 100 $generic_sg_noun
  NN 100 $generic_mass_noun
  NNS 100 $generic_pl_noun
  NNPS 100 $generic_pl_noun
  NNP 100 $genericname
  FW 100 $generic_mass_noun
  RB 100 $generic_adverb
  VB 100 $generic_trans_verb_bse
  VBD 100 $generic_trans_verb_past
  VBG 100 $generic_trans_verb_prp
  VBN 100 $generic_trans_verb_psp
  VBP 100 $generic_trans_verb_presn3sg
  VBZ 100 $generic_trans_verb_pres3sg
. 

;;
;; discount on priority for words covered by multi-word lexical entry
;;
lexical-covering-discount := -200.

;;
;; discount on priority for POS-supplied lexical entries.
;;
posdiscount := SpellCorrection -200.

;;;
;;; the setting `pos-completion' enables an additional mechanism:
;;; whenever we receive POS information as part of the input, we check
;;; to see whether the built-in lexical entries suffice to satisfy the
;;; POS annotations: each lexical entry retrieved for an input token
;;; <string, pos_1, pos_2, pos_3> is mapped to an application-specific
;;; POS tag, using the `type-to-pos' map, and checking the type of each
;;; lexical entry for subsumption against the left-hand side of each
;;; `type-to-pos' rule.  some or all POS annotations from the input may
;;; be `satisfied' under this mapping by built-in lexical entries,
;;; e.g. for the example above, there may be lexical entries whose type
;;; maps to `pos_1' and `pos_3'.  unless all POS annotations are
;;; satisfied after all built-in lexical entries have been processed,
;;; the remaining POS categories are processed by the regular
;;; `posmapping' look-up.  however, the resulting generic lexical
;;; entries will have their scores discounted by the value of
;;; `discount-gen-le-priority', so as to distinguish them from
;;; completely unknown words.  note that, as a side effect, an empty
;;; `type-to-pos' map will always result in activating all generic
;;; lexical entries, even for input tokens that were found in the
;;; built-in lexicon.
;;;
;pos-completion.

type-to-pos :=
  basic_noun_word NN
  basic_noun_word NNS
  basic_noun_word NNP
  basic_pronoun_word NN
  basic_pronoun_word NNS
  basic_pronoun_word NNP
;  basic_det_word UpperAndLowerCase
;  basic_prep_word UpperAndLowerCase
.

discount-gen-le-priority := 10.

;;;
;;; from here on, scoring for grammar rules and lexical entries.
;;;

default-rule-priority := 500.

rule-priorities :=
;  $extradj_s 150
  $extracomp 400
  $extrasubj_f 350
  $extrasubj_i 300
  $fillhead_non_wh 300
  $fillhead_wh_r 400
  $fillhead_wh_subj_r 350
  $fillhead_wh_nr_f 300
  $fillhead_wh_nr_i 150
  $fillhead_rel 250
  $freerel_inf 100
  $freerel_fin 200
  $hoptcomp 400
  $hopt2comp 900
  $rootgap_l 150
;  $rootgap_rs 150
  $np_n_cmpnd 300
  $np_n_cmpnd_2 300
  $noun_n_cmpnd 0
  $nadj_rc 400
  $nadj_rr_nt 0
  $nadj_rr_t 300
  $adjh_i 450
  $top_coord_e 600
  $mid_coord_e 200
  $mid_coord_nom 150
  $subjh 600
  $hcomp 900
  $hspec 600
;  $hadj_nv_uns 550
;  $hadj_v_uns 40
;  $hadj_i_uns_aux 150
  $hadj_s 400
  $bare_np 500
  $bare_vger 500
  $proper_np 800
  $noptcomp 0
;  $extradj_i_aux_subj 150
;  $extradj_i_aux_nosubj 150
;  $extradj_t_aux 50
  $fin_non_wh_rel 300
  $inf_non_wh_rel 100
  $vpellipsis_ref_lr 100
  $vpellipsis_expl_lr 100
  $taglr 300
  $vgering 300
  $measure_np 550
  $meas_np_adj 550
  $appos 150
  $imper 500
;  $tempnp_wh 800
;  $tempnp_nwh 350
;  $root_cl 800
;  $partnum 600
  $sailr 400
  $advadd 700
  $num_noun 300
  $passive 500
  $intransng 310
  $transng 300
  $monthdet 400
  $weekdaydet 400
  $attr_adj 600
  $attr_adj_verb_part 100
  $part_ppof_agr 200
  $part_ppof_noagr 200
  $part_nocomp 250
  $np_part_lr 400
  $dative_lr 700
  $plur_noun_infl_rule 0
  $third_sg_fin_verb_infl_rule 0
  $past_verb_infl_rule 0
  $psp_verb_infl_rule 0
  $prp_verb_infl_rule 0
  $non_third_sg_fin_verb_infl_rule 0
  $bse_verb_infl_rule 0
  $sing_noun_infl_rule 0
  $mass_noun_infl_rule 0
  $mass_count_infl_rule 0
  $pos_adj_infl_rule 0
  $mv_sorb_pass_infl_rule 0
  ; DPF 1-Mar-02 - Comment out the rest for ec test run
;  $possessed_word_lr 50
;  $bare_np_sg 350
;  $runon_s 30
;  $nocop_vp 600
;  $nocop_id_vp 150
;  $nocop_s 450
.

spanning-only-rules := $runon_s $punct_conj.

default-le-priority := 800.

unlikely-le-types := 
  n_mealtime_le 200
  $here_nom 800
  n_adv_le 300
  n_adv_acc_le 300
  n_adv_simp_acc_le 300
  n_adv_event_le 300
  n_adv_too_le 300
  vc_there_is_le 350
  vc_there_are_le 350
  vc_there_was_le 350
  vc_there_were_le 350
  vc_id_be_le 300
  vc_id_am_le 200
  vc_id_am_cx_le 200
  vc_id_are_le 300
  vc_id_are_neg_le 300
  vc_id_been_le 300
  vc_id_is_le 300
  vc_id_is_neg_le 300
  vc_id_was_le 300
  vc_id_was_neg_le 300
  vc_id_were_le 300
  vc_id_were_neg_le 300
;  adv_int_vp_post_le 200
  n_freerel_pro_le 200
  n_freerel_pro_adv_le 200
  v_sorb_le 200
  det_part_one_le 200
;  $like_disc_adv 200
  adv_disc_please_le 500
  adv_disc_le 200
  adv_disc_preh_le 200
;  p_cp_le 200
  $show_v6 900
  v_subj_equi_prd_le 250
  v_subj_equi_prd_prep_le 250
  v_subj_equi_prd_verb_le 250
  comp_to_prop_elided_le 200
  comp_to_nonprop_elided_le 200
  n_hour_prep_le 200
  n_pers_pro_noagr_le 200
  n_proper_abb_le 10
  v_ditrans*_only_le 200
  $that_deix 90
  ; 10-Apr-02 - Was 200: This is my polite way of saying that I do not want to do business with your company
  N_DEICTIC_PRO_SG_LE 300
  N_DEICTIC_PRO_PL_LE 300
  ;V_SSR_MEDPRIO_LE 300
  V_UNACC_LE 350
  ;N_INTR_LOWPRIO_LE 100
  P_NBAR_COMP_LE 200
  P_NBAR_COMP_NMOD_LE 40
  N_YEAR_LE 400
  ;V_PREP_INTRANS_MEDPRIO_LE 300
  ;N_MASS_LOWPRIO_LE 10
  ;N_MASS_COUNT_PPOF_LOWPRIO_LE 100
  ;N_PROPER_LOWPRIO_LE 200
  ;V_OBJ_EQUI_NON_TRANS_PRD_LOWPRIO_LE 200
  ;V_OBJ_EQUI_PRD_MEDPRIO_LE 200
  PP_WH_LE 200
  VC_ID_AM_LE 200
  ;V_PARTICLE_NP_MEDPRIO_LE 300
  ;V_NP_NON_TRANS_MEDPRIO_LE 400
  ; 11/14 was 200: "I haven't gotten my order which i ordered more than two weeks
  ; ago"  Can't be 250: "I got here"
  ;V_NP_NON_TRANS_LOWPRIO_LE 220
  ;ADJ_REG_ATRANS_MEDPRIO_LE 250
  ;ADJ_INTRANS_LOWPRIO_LE 200
  ;ADJ_INTRANS_MEDPRIO_LE 400
  ;ADJ_PRED_INTRANS_LOWPRIO_LE 200
  ;ADJ_ATTR_INTRANS_LOWPRIO_LE 200
  V_SOR_NON_TRANS_LE 500
  ;V_UNERG_MEDPRIO_LE 300
  ;V_UNERG_IMP_MEDPRIO_LE 300
  ; 11/15 was 250 - i know when my order was sent
  ;P_SUBCONJ_LOWPRIO_LE 150
  EHEADER_VERB_NON_NP_LE 500
  ADJ_INTRANS_NALE 50
  ADV_WORD_NALE 10 
  ;P_REG_MEDPRIO_LE 450
  ;P_MEDPRIO_LE 450
  ;N_PPOF_MEDPRIO_LE 400
  ;N_PPOF_LOWPRIO_LE 150
  ; 2Nov01 - Was 450: Send all application forms
  N_PART_NPCOMP_AGR_LE 220
  ; 2Nov01 - Was 450: Send all application forms
  N_PART_NPCOMP_NOAGR_LE 220
  ;ADV_S_PRE_WORD_NOSPEC_MEDPRIO_LE 500
  ; These were 110, 120, 210
  N_PROPER_NALE 210
  N_INTR_SG_NALE 220
  N_INTR_MASS_NALE 220
  N_INTR_PL_NALE 310
  N_X_TO_Y_SG_LE 10
  ;P_SUBCONJ_IF_INDIC_LE 500
  ;PP_NMOD_LOWPRIO_LE 100
  ;V_NP_PREP_TRANS_NOMSG_MEDPRIO_LE 200
  ;V_NP_PREP_TRANS_NOMSG_LOC_MEDPRIO_LE 200
  ;V_PARTICLE_MEDPRIO_LE 200
  ;N_PLUR_MEDPRIO_LE 250
  ;N_PLUR_LOWPRIO_LE 100
  ;ADJ_TRANS_MEDPRIO_LE 300
  ;ADJ_COMP_MEDPRIO_LE 300
  ;ADJ_SUPERL_MEDPRIO_LE 300
  V_CP_SUBJ_LE 300
  ; 10-Apr-02 - Was 200: Her son's tenth birthday arrived
  N_POSS_CLITIC_PHR_LE 250
  P_NO_NMOD_LE 40
  ADJ_ONE_SING_PRD_LE 150
.

likely-le-types := 
  $like_v2 400
  $have-prd_in 900
  $let_v1 900
  $then_disc_pre 800
;  $out_of_p 900
;  $would_like_v4 600
  $about 300
  $abstain_v2 300
  $acceptable_a2 250
  $adapt_v2 300
  $agree_v2 300
  $aim_v2 300
  $alternate_v2 300
  $approve_v2 300
  $arise_v1 300
  $bargain_v3 300
  $become_v2 400
  $believe_v2 300
  $belong_v2 300
  $but_adv1 500
  $buy_v1 250
  $call_v1 250
  $change_v1 250
  $charge_v1 250
  $check_v2 300
  $coincide_v1 300
  $come_v1 300
  $compare_v1 250
  $comply_v1 300
  $decide_v2 300
  $differ_v2 300
  $difficult_a2 250
  $disagree_v1 300
  $exchange_v1 250
  $expect_v1 250
  $expect_v4 300
  $fail_v1 250
  $fail_v3 300
  $fax_v2 250
  $find_out_v4 200
;  $find_v1 250
  $focus_v2 300
  $forget_v3 300
;  $get_v3 200
  $give_v2 250
  $go_v1 300
  $good_a2 400
  $happen_v2 300
  $hear_v4 300
  $important_a2 250
  $impossible_a2 250
  $insist_v4 300
  $interest_in_v1 700
  $interested_a2 400
  $interfere_v1 300
  $know_v5 300
  $later_a1 300
  $latest_a1 300
  $leave_v2 250
  $let_v2 400
  $like_v1 250
  $like_v2 400
  $like_v3 500
  $like_v4 400
  $likely_a2 250
  $listen_v2 300
  $lose_v1 300
  $make_v4 200
  $middle_n1 400
  $move_v3 250
  $necessary_a2 250
  $negotiate_v2 300
  $object_v1 300
  $occur_v1 300
  $okay_a2 250
;  $order_v1 250
  $out_p 450
  $overlap_v2 300
  $participate_v1 300
  $possible_a2 250
  $prevent_v1 250
  $proceed_v1 300
  $put_through_v2 300
  $reasonable_a2 250
  $recuperate_v2 300
  $regards_n1 250
  $remove_v1 250
  $replace_v1 250
  $rush_v2 250
  $see_v3 300
  $send_v1 350
  $send_v2 300
;  $send_v4 200
  $send_v5 450
  $shoot_v2 300
  $shop_v1 300
  $show_v5 300
  $shut_v1 250
  $ski_v1 300
;  $so_adv1 500
  $succeed_v1 300
  $switch_v2 250
  $take_v1 250
  $talk_v2 300
;  $thanks_v2 300
  $to 450
  $unlikely_a2 250
  $vote_v1 300
  $wait_v1 300
  $worry_v2 300
  $wrong_a2 400
  $east_n2 500
  $north_n2 500
  $south_n2 500
  $west_n2 500
  $complain_v1 300
  $concentrate_v2 300
  $consent_v2 300
  $cope_v2 300
  $flee_v2 300
  $inquire_v3 300
  $notify_v1 250
  $refrain_v2 300
  $withdraw_v13 250
  $crucial_a2 250
  $encouraging_a2 250
  $interesting_a2 250
  $optional_a2 250
  $profitable_a2 250
  $satisfactory_a2 250
  $unnecessary_a2 250
  $forward_v1 250
  $interest_v1 250
  $lecture_v1 300
  $limit_down_v2 300
  $look_v1 300
  $point_out_v2 300
  $trade_in_v2 300
  $discourage_v1 250
  $transfer_v2 250
  $credit_v2 250
  $plunge_v1 300
  $lean_v2 300
  $chat_v2 300
  $out_a1 300
  $delete_v1 250
  $unsubscribe_v1 250
  $respond_v2 300
  $return_v1 250
  $return_v2 300
  $southeast_n2 500
  $southwest_n2 500
  $northeast_n2 500
  $northwest_n2 500
  $supply_v1 250
  $bombard_v1 250
  $comment_v2 300
  $communicate_v1 300
  $deluge_v1 250
  $default_v2 300
  $invoice_v1 250
  $project_v1 250
  $rescue_v1 250
  $qualify_v2 300
  $prequalify_v2 300
  $resend_v1 250
  $ship_v2 250
  $write_v4 300
  $total_v1 400
  $feed_v2 250
  $enquire_v2 300
  $complicated_a2 250
  $vital_a2 250
  $urgent_a2 250
  $unsatisfactory_a2 250
  $call_v3 200
  $forbid_v1 250
  $schlep_v1 250
  $edit_n1 400
  $acceptable_a1 200
  $add_v1 120
  $advance_v2 200
  $assure_v1 200
  $available_a2 100
  $Bill 200
  $book_v1 200
  $bring_v3 200
  $call_ditrans_v1 200
  $charge_v3 200
  $consider_v3 200
  $declare_v6 200
  $difficult_a1 200
  $do2 300
  $do_v4 200
  $get_v1 300
  $get_v2 300
  $get_prd_v2 200
  $give_v1 200
  $guy_n1 100
  $have-prd 50
  $important_a1 200
  $impossible_a1 200
  $level_a1 200
  $level_v1 120
  $like_prd_v1 200
  $likely_a1 200
  $make_v3 200
  $necessary_a1 200
;  $need_v1 120
  $need-prd 50
  $want_prd 50
  $offer_v1 200
  $okay_a1 200
  $please_v1 120
  $possible_a1 200
  $prepare_v3 200
  $reasonable_a1 200
  $sell_v1 200
  $shift_v2 120
  $tell_v4 120
  $thanks_n1 100
  $think_v3 220
  $unlikely_a1 200
  $US_n1 100
  $back_n1 150
  $finding_n1 100
  $left_n1 10
  $regard_n1 150
  $right_n1 100
  $crucial_a1 200
  $encouraging_a1 200
  $interesting_a1 200
  $optional_a1 200
  $profitable_a1 200
  $unnecessary_a1 200
  $cost_v1 200
  $in_a1 200
  $need_n1 50
  $through_a1 200
  $thru_a1 200
  $on_a1 200
  $used_a2 200
  $will_n1 100
  $number_v1 120
  $mail_v2 120
;  $return_v5 200
  $electric_n1 10
  $attempt_v1 120
  $leave_n1 10
  $resend_v2 200
  $ship_v3 200
;  $order_n2 10
  $cloud_v1 120
  $synchronize_v3 120
  $so_a1 200
  $wonder_v3 120
  $crown_v2 200
  $freeze_v1 120
  $pocket_v1 120
  $hold_up_n1 100
  $complicated_a1 200
  $vital_a1 200
  $urgent_a1 200
  $unsatisfactory_a1 200
  $fed_ex_v2 200
  $fed_ex_v2a 200
  $still_not 500
  $following_n1 150
  $profile_n1 150
  $after_pp 200
  $request_v3 300
  $like_p 200
  $no_det 450
  $wish_v4 200
  $number_abb_n1 100
;  $shipping_n1 900
  $before_adv1 300
  $after_pp 300
  $look_seem_v1 200
  $checking_n1 300
  $clear_v1 300
  CONJ_COMPLEX_AND_LE 900
  CONJ_COMPLEX_AND_BOTH_LE 900
  CONJ_COMPLEX_AND_THEN_LE 900
  CONJ_COMPLEX_THEN_LE 900
  CONJ_COMPLEX_AS_WELL_AS_LE 900
  CONJ_COMPLEX_AS_WELL_AS_BOTH_LE 900
  CONJ_COMPLEX_BUT_LE 900
  CONJ_COMPLEX_NOR_LE 900
  CONJ_COMPLEX_NOR_NEITHER_LE 900
  CONJ_COMPLEX_OR_LE 900
  CONJ_COMPLEX_OR_EITHER_LE 900
  CONJ_COMPLEX_PLUS_LE 900
  CONJ_COMPLEX_SO_LE 900
  CONJ_COMPLEX_VS_LE 900
  CONJ_COMPLEX_MINUS_LE 900
  CONJ_COMPLEX_BUT_NOT_LE 900
  CONJ_COMPLEX_AMP_LE 900
  CONJ_COMPLEX_AMP_BOTH_LE 900
  CONJ_COMPLEX_RATHER_LE 900
  CONJ_COMPLEX_AND_OR_LE 900
  VA_QUASIMODAL_LE 900
  V_POSS_LE 400
  N_HOUR_LE 900
  P_DITRANS_LE 900
  P_DITRANS_FROM_TO_LE 900
  V_EXPL_IT_SUBJ_LIKE_LE 900
  ADV_S_PRE_WORD_NOSPEC_LE 900
  ADJ_MORE_LESS_LE 900
  ADJ_MOST_LEAST_LE 900
  V_SUBJ_EQUI_LE 850
  V_SUBJ_EQUI_NONFIN_LE 850
  ;V_SUBJ_EQUI_MEDPRIO_LE 400
  ;V_SUBJ_EQUI_PRP_MEDPRIO_LE 500
  N_PROPER_LE 900
  ; V_PREP_PARTICLE_NP_LE 900
  N_WH_PRO_LE 900
  N_WH_PRO_ACC_LE 900
  N_REL_PRO_WHAT_LE 900
  N_REL_PRO_WHO_LE 900
  N_REL_PRO_ACC_LE 900
  V_EMPTY_PREP_INTRANS_LE 900
  ;V_EMPTY_PREP_INTRANS_IMP_LE 900
  COMP_TO_PROP_LE 900
  COMP_TO_NONPROP_LE 900
  COMP_IF_INDIC_LE 900
  CONJ_AND_NUM_LE 900
  ADJ_COMPLEMENTED_UNSPECIFIED_CARD_LE 900
  ADJ_REG_EQUI_LE 900
  V_NP_PREP_TRANS_LE 900
  V_NP_PREP_TRANS_DORS_LE 900
  ; V_NP_PREP*_TRANS_LE 900
  V_NP*_PREP_TRANS_LE 900
  P_SUBCONJ_LE 900
  ; DPF 10-Feb-02
  P_SUBCONJ_INF_LE 300
  ; V_EMPTY_PREP*_TRANS_NOSUBJ_LE 900
  V_DOUBLE_PP_LE 900
  V_NP*_TRANS_PRP_NALE 100
  V_NP*_TRANS_PRES3SG_NALE 100
  V_NP*_TRANS_PAST_NALE 100
  V_NP*_TRANS_PSP_NALE 100
  ADJ_TRANS_LE 900
  ADJ_TRANS_OBLIG_LE 900
  N_TITLE_LE 900
  ADJ_REG_ATRANS_CP_LE 900
  ADJ_REG_ATRANS_LE 900
  V_OBJ_EQUI_NON_TRANS_PRD_IMPER_LE 900
  ;V_SUBJ_EQUI_AND_LE 900
  ;V_SOR_NON_TRANS_HIPRIO_LE 600
  ;V_SOR_NON_TRANS_MEDPRIO_LE 400
  DET_PART_MS_WH_LE 900
  DET_PART_PL_WH_LE 900
  V_NP_TRANS_DOUBLE_PP*_LE 900
  V_NP*_TRANS_DOUBLE_PP*_LE 900
  V_NP*_TRANS_DOUBLE_PP_TO*_LE 900
  V_EXPL_IT_SUBJ_NP_CP_LE 900
  V_EXPL_IT_SUBJ_NP_CP_FIN_LE 900
  V_EXPL_IT_SUBJ_NP_NP_CP_INF_LE 900
  EHEADER_VERB_NP_LE 800
  ;ADV_DISC_HIPRIO_LE 800
  ;ADV_DISC_PREH_HIPRIO_LE 800
  N_X_TO_Y_PLUR_LE 900
  N_MASS_COUNT_PPCOMP_LE 500
  V_EMPTY_TO_TRANS_LE 900
  N_GENERIC_PRO_ADV_LE 900
  V_CP_INF_LE 900
  V_PARTICLE_PP_LE 900
  COMP_FOR_LE 900
  ;P_REG_HIPRIO_LE 900
  ;V_NP_TRANS_HIPRIO_LE 900
.

;;;
;;; based on selectional dependencies between lexical entries, reduce chart
;;; right after lexical look-up: `chart-dependencies' is a list of pairs of
;;; paths into lexical entries.  the type of the node at the end of the first
;;; path in one lexical entry makes that entry depend on the existence of some
;;; other lexical entry that has that same type as the value of the node at the
;;; end of the second path.
;;;
;;; _fix_me_
;;; not entirely sure, but it must (in principle) be possible to saturate a
;;; dependency from lexical and grammar rules.  say, a lexical entry selected
;;; for something nominalized, and that relation was introduced by a lexical
;;; rule, in turn.  unless this is the case already, compute static list of all
;;; relations introduced by rules (which, presumably, requires another setting
;;; to declare how to find constructional semantic contributions; C-CONT) and
;;; consider all such dependencies on lexical entries always saturated.
;;;                                                          (11-oct-02; oe)
unidirectional-chart-dependencies.
chart-dependencies := 
;  "SYNSEM.LOCAL.CAT.VAL.SUBJ.FIRST.LOCAL.CONT.INDEX"
;  "SYNSEM.LOCAL.CONT.INDEX"
  "SYNSEM.LKEYS.--+COMPKEY" "SYNSEM.LOCAL.KEYS.KEY"
  "SYNSEM.LKEYS.--+OCOMPKEY" "SYNSEM.LOCAL.KEYS.KEY"
.
