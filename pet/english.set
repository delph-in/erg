;;;
;;; This file is part of the English Level 2 Language model.  It should be
;;; kept in the same directory as the accompanying files `english.grm' and 
;;; `l2global.set'.  Do not read any further; if you feel you have to, do not,
;;; under no circumstances, whatsover, change any of the following.
;;;

encoding := iso-8859-1.

include "global".
include "k2y".

;;
;; type of a valid parse --- used only for CSLI test suite
;;
start-symbols := $root_strict.

;;
;; word-breaking characters in tokenization --- treated as whitespace
;;
;;punctuation-characters := "?!.:;,()<>{}[]+*-`_\"".
punctuation-characters := ",()<>{}[]+*`_\"".

;;
;; weighted start symbols: pairs of root type plus threshold; only once the
;; top score on the agenda drops below the threshold, the associated root will
;; be allowed to license results.  when the search space is exhausted, the
;; agenda score drops to 0, eventually.
;;
;; when set, `weighted-start-symbols' takes precedence over `start-symbols'.
;;
#|
weighted-start-symbols := 
  $root_strict 1000 
  $root_lex 1000
  $root_gap 800
  $root_conj 200
  $root_phr 50
  $root_subord 50
.
|#

;;
;; suppress results of regular morphology rules if irregular form exists
;;
irregular-forms-only.

lex-entries-can-fail.

;;
;; names of attributes not to pass from daughter to mother in parsing
;;
deleted-daughters := ARGS HEAD-DTR NON-HEAD-DTR LCONJ-DTR RCONJ-DTR.

;;
;; names of attributes to filter for packing parser
;;
packing-restrictor := RELS HCONS.

;;
;; path into the MRS semantics
;;
mrs-path := "SYNSEM.LOCAL.CONT".

;;
;; path to LABEL for relation-to-word mapping, absolute and within a relation
;;
label-path := "SYNSEM.LOCAL.KEYS.KEY.LABEL".
label-path-tail := "LABEL".

;;
;; name of type containing quick check structure
;;
qc-structure := $qc_paths.

;;;
;;; generic lexical entries for unknown words: for each unknown word in the
;;; input all generic entries are postulated.  those that require a certain
;;; suffix (`generic-le-suffixes') only fire if the input form has the suffix.
;;; scoring for generic items is based on the default priority (typically 
;;; fairly low) and (optionally) adjusted on the basis of POS information that
;;; may be available for the unknown word.  if the input word has one more more
;;; POS tags associated to it, these are looked up in the `posmapping' table:
;;; this table is a list of triples (tag, score, gle) where `gle' is the name
;;; of one of the generic items in `generic-les'.  for each generic item, the
;;; score is adjusted to the first match of one of the tags associated with the
;;; unknown word in the mapping table.
;;;

generic-lexentry-status-values := generic-lex-entry.

;;;
;;; default initial priority assigned to generic lexical entries; this may be
;;; re-adjusted by later steps in unknown word handling.  generic entries that
;;; end up with a zero priority are dropped before parsing.
;;;
default-gen-le-priority := 0.

;;;
;;; some generic lexical entries require inflectional marking.  this mechanism
;;; is a filter on which generic entries proposed by other means can survive:
;;; generic entries listed here will only be postulated if the required suffix 
;;; can be matched against the input token.
;;;
;;; when using only the generic entries licensed by a POS tag (i.e. a default
;;; priority of 0 and a non-empty `posmapping'), the suffix filter really does
;;; not make a lot of sense anymore.
;;;
generic-le-suffixes := 
#|
  $generic_trans_verb_pres3sg "S" 
  $generic_trans_verb_past "ED" 
  $generic_trans_verb_psp "ED" 
  $generic_trans_verb_prp "ING" 
  $generic_pl_noun "S"
|#
.

posmapping := 
  UpperAndLowerCase 800 $genericname
  UpperAndLowerCaseInitial 200 $genericname
  JJ 100 $generic_adj
  JJR 100 $generic_adj_compar
  JJS 100 $generic_adj_superl
  NN 100 $generic_sg_noun
  NN 100 $generic_mass_noun
  NNS 100 $generic_pl_noun
  NNPS 100 $generic_pl_noun
  NNP 100 $genericname
  FW 100 $generic_mass_noun
  RB 100 $generic_adverb
  VB 100 $generic_trans_verb_bse
  VBD 100 $generic_trans_verb_past
  VBG 100 $generic_trans_verb_prp
  VBN 100 $generic_trans_verb_psp
  VBP 100 $generic_trans_verb_presn3sg
  VBZ 100 $generic_trans_verb_pres3sg
. 

;;
;; discount on priority for words covered by multi-word lexical entry
;;
lexical-covering-discount := -200.

;;
;; discount on priority for POS-supplied lexical entries.
;;
posdiscount := SpellCorrection -200.

;;;
;;; the setting `pos-completion' enables an additional mechanism:
;;; whenever we receive POS information as part of the input, we check
;;; to see whether the built-in lexical entries suffice to satisfy the
;;; POS annotations: each lexical entry retrieved for an input token
;;; <string, pos_1, pos_2, pos_3> is mapped to an application-specific
;;; POS tag, using the `type-to-pos' map, and checking the type of each
;;; lexical entry for subsumption against the left-hand side of each
;;; `type-to-pos' rule.  some or all POS annotations from the input may
;;; be `satisfied' under this mapping by built-in lexical entries,
;;; e.g. for the example above, there may be lexical entries whose type
;;; maps to `pos_1' and `pos_3'.  unless all POS annotations are
;;; satisfied after all built-in lexical entries have been processed,
;;; the remaining POS categories are processed by the regular
;;; `posmapping' look-up.  however, the resulting generic lexical
;;; entries will have their scores discounted by the value of
;;; `discount-gen-le-priority', so as to distinguish them from
;;; completely unknown words.  note that, as a side effect, an empty
;;; `type-to-pos' map will always result in activating all generic
;;; lexical entries, even for input tokens that were found in the
;;; built-in lexicon.
;;;
;pos-completion.

type-to-pos :=
  basic_noun_word NN
  basic_noun_word NNS
  basic_noun_word NNP
  basic_pronoun_word NN
  basic_pronoun_word NNS
  basic_pronoun_word NNP
;  basic_det_word UpperAndLowerCase
;  basic_prep_word UpperAndLowerCase
.

discount-gen-le-priority := 10.

;;;
;;; from here on, scoring for grammar rules and lexical entries.
;;;

default-rule-priority := 500.

rule-priorities :=
;  $extradj_s 150
  $extracomp 400
  $extrasubj_f 350
  $extrasubj_i 300
  $fillhead_non_wh 300
  $fillhead_wh_r 400
  $fillhead_wh_subj_r 350
  $fillhead_wh_nr_f 300
  $fillhead_wh_nr_i 150
  $fillhead_rel 250
  $freerel_inf 100
  $freerel_fin 200
  $hoptcomp 400
  $hopt2comp 900
  $rootgap_l 150
;  $rootgap_rs 150
  $np_n_cmpnd 300
  $np_n_cmpnd_2 300
  $noun_n_cmpnd 0
  $nadj_rc 400
  $nadj_rr_nt 0
  $nadj_rr_t 300
  $adjh_i 450
  $top_coord_e 600
  $mid_coord_e 200
  $mid_coord_nom 150
  $subjh 600
  $hcomp 900
  $hspec 600
;  $hadj_nv_uns 550
;  $hadj_v_uns 40
;  $hadj_i_uns_aux 150
  $hadj_s 400
  $bare_np 500
  $bare_vger 500
  $proper_np 800
  $noptcomp 0
;  $extradj_i_aux_subj 150
;  $extradj_i_aux_nosubj 150
;  $extradj_t_aux 50
  $fin_non_wh_rel 300
  $inf_non_wh_rel 100
  $vpellipsis_ref_lr 100
  $vpellipsis_expl_lr 100
  $taglr 300
  $vgering 300
  $measure_np 550
  $meas_np_adj 550
  $appos 150
  $imper 500
;  $tempnp_wh 800
;  $tempnp_nwh 350
;  $root_cl 800
;  $partnum 600
  $sailr 400
  $advadd 700
  $num_noun 300
  $passive 500
  $intransng 310
  $transng 300
  $monthdet 400
  $weekdaydet 400
  $attr_adj 600
  $attr_adj_verb_part 100
  $part_ppof_agr 200
  $part_ppof_noagr 200
  $part_nocomp 250
  $np_part_lr 400
  $dative_lr 700
  $plur_noun_infl_rule 0
  $third_sg_fin_verb_infl_rule 0
  $past_verb_infl_rule 0
  $psp_verb_infl_rule 0
  $prp_verb_infl_rule 0
  $non_third_sg_fin_verb_infl_rule 0
  $bse_verb_infl_rule 0
  $sing_noun_infl_rule 0
  $mass_noun_infl_rule 0
  $mass_count_infl_rule 0
  $pos_adj_infl_rule 0
  $mv_sorb_pass_infl_rule 0
  ; DPF 1-Mar-02 - Comment out the rest for ec test run
;  $possessed_word_lr 50
;  $bare_np_sg 350
;  $runon_s 30
;  $nocop_vp 600
;  $nocop_id_vp 150
;  $nocop_s 450
.

spanning-only-rules := $runon_s $punct_conj.

default-le-priority := 800.

unlikely-le-types := 
  n_mealtime_le 200
  $here_nom 800
  n_adv_le 300
  n_adv_acc_le 300
  n_adv_simp_acc_le 300
  n_adv_event_le 300
  n_adv_too_le 300
  vc_there_is_le 350
  vc_there_are_le 350
  vc_there_was_le 350
  vc_there_were_le 350
  vc_id_be_le 300
  vc_id_am_le 200
  vc_id_am_cx_le 200
  vc_id_are_le 300
  vc_id_are_neg_le 300
  vc_id_been_le 300
  vc_id_is_le 300
  vc_id_is_neg_le 300
  vc_id_was_le 300
  vc_id_was_neg_le 300
  vc_id_were_le 300
  vc_id_were_neg_le 300
;  adv_int_vp_post_le 200
  n_freerel_pro_le 200
  n_freerel_pro_adv_le 200
  v_sorb_le 200
  det_part_one_le 200
;  $like_disc_adv 200
  adv_disc_please_le 500
  adv_disc_le 200
  adv_disc_preh_le 200
;  p_cp_le 200
  $show_v6 900
  v_subj_equi_prd_le 250
  v_subj_equi_prd_prep_le 250
  v_subj_equi_prd_verb_le 250
  comp_to_prop_elided_le 200
  comp_to_nonprop_elided_le 200
  n_hour_prep_le 200
  n_pers_pro_noagr_le 200
  n_proper_abb_le 10
  v_ditrans*_only_le 200
  $that_deix 90
  n_deictic_pro_sg_le 300
  n_deictic_pro_pl_le 300
  v_unacc_le 350
  p_nbar_comp_le 200
  p_nbar_comp_nmod_le 40
  n_year_le 400
  pp_wh_le 200
  vc_id_am_le 200
  v_sor_non_trans_le 500
  eheader_verb_non_np_le 500
  n_part_npcomp_agr_le 220
  n_part_npcomp_noagr_le 220
  n_x_to_y_sg_le 10
;  p_subconj_if_indic_le 500
  v_cp_subj_le 300
  n_poss_clitic_phr_le 250
  p_no_n_mod_le 40
  adj_one_sing_prd_le 150
  ;;
  ;; types for (default set of) generic lexical entries: these determine the
  ;; priorities assigned to lexical entries retrieved for unknown words; note
  ;; that additional discounting and boosting mechanisms may further adjust
  ;; those priorities (see section on unknown word handling above).
;  adj_intrans_nale 50
;  adv_word_nale 10 
;  n_proper_nale 210
;  n_intr_sg_nale 220
;  n_intr_mass_nale 220
;  n_intr_pl_nale 310
;  v_np*_trans_prp_nale 100
;  v_np*_trans_pres3sg_nale 100
;  v_np*_trans_past_nale 100
;  v_np*_trans_psp_nale 100
.

likely-le-types := 
  $like_v2 400
  $have-prd_in 900
  $let_v1 900
  $then_disc_pre 800
;  $out_of_p 900
;  $would_like_v4 600
  $about 300
  $abstain_v2 300
  $acceptable_a2 250
  $adapt_v2 300
  $agree_v2 300
  $aim_v2 300
  $alternate_v2 300
  $approve_v2 300
  $arise_v1 300
  $bargain_v3 300
  $become_v2 400
  $believe_v2 300
  $belong_v2 300
  $but_adv1 500
  $buy_v1 250
  $call_v1 250
  $change_v1 250
  $charge_v1 250
  $check_v2 300
  $coincide_v1 300
  $come_v1 300
  $compare_v1 250
  $comply_v1 300
  $conflict_v2 300
  $decide_v2 300
  $differ_v2 300
  $difficult_a2 250
  $disagree_v1 300
  $exchange_v1 250
  $expect_v1 250
  $expect_v4 300
  $fail_v1 250
  $fail_v3 300
  $fax_v2 250
  $find_out_v4 200
;  $find_v1 250
  $focus_v2 300
  $forget_v3 300
;  $get_v3 200
  $give_v2 250
  $go_v1 300
  $good_a2 400
  $happen_v2 300
  $hear_v4 300
  $important_a2 250
  $impossible_a2 250
  $insist_v4 300
  $interest_in_v1 700
  $interested_a2 400
  $interfere_v1 300
  $know_v5 300
  $later_a1 300
  $latest_a1 300
  $leave_v2 250
  $let_v2 400
  $like_v1 250
  $like_v2 400
  $like_v3 500
  $like_v4 400
  $likely_a2 250
  $listen_v2 300
  $lose_v1 300
  $make_v4 200
  $middle_n1 400
  $move_v3 250
  $necessary_a2 250
  $negotiate_v2 300
  $object_v1 300
  $occur_v1 300
  $okay_a2 250
;  $order_v1 250
  $out_p 450
  $overlap_v2 300
  $participate_v1 300
  $possible_a2 250
  $prevent_v1 250
  $proceed_v1 300
  $put_through_v2 300
  $reasonable_a2 250
  $recuperate_v2 300
  $regards_n1 250
  $remove_v1 250
  $replace_v1 250
  $rush_v2 250
  $see_v3 300
  $send_v1 350
  $send_v2 300
;  $send_v4 200
  $send_v5 450
  $shoot_v2 300
  $shop_v1 300
  $show_v5 300
  $shut_v1 250
  $ski_v1 300
;  $so_adv1 500
  $succeed_v1 300
  $switch_v2 250
  $take_v1 250
  $talk_v2 300
;  $thanks_v2 300
  $to 450
  $unlikely_a2 250
  $vote_v1 300
  $wait_v1 300
  $worry_v2 300
  $wrong_a2 400
  $east_n2 500
  $north_n2 500
  $south_n2 500
  $west_n2 500
  $complain_v1 300
  $concentrate_v2 300
  $consent_v2 300
  $cope_v2 300
  $flee_v2 300
  $inquire_v3 300
  $notify_v1 250
  $refrain_v2 300
  $withdraw_v13 250
  $crucial_a2 250
  $encouraging_a2 250
  $interesting_a2 250
  $optional_a2 250
  $profitable_a2 250
  $satisfactory_a2 250
  $unnecessary_a2 250
  $forward_v1 250
  $interest_v1 250
  $lecture_v1 300
  $limit_down_v2 300
  $look_v1 300
  $point_out_v2 300
  $trade_in_v2 300
  $discourage_v1 250
  $transfer_v2 250
  $credit_v2 250
  $plunge_v1 300
  $lean_v2 300
  $chat_v2 300
  $out_a1 300
  $delete_v1 250
  $unsubscribe_v1 250
  $respond_v2 300
  $return_v1 250
  $return_v2 300
  $southeast_n2 500
  $southwest_n2 500
  $northeast_n2 500
  $northwest_n2 500
  $supply_v1 250
  $bombard_v1 250
  $comment_v2 300
  $communicate_v1 300
  $deluge_v1 250
  $default_v2 300
  $invoice_v1 250
  $project_v1 250
  $rescue_v1 250
  $qualify_v2 300
  $prequalify_v2 300
  $resend_v1 250
  $ship_v2 250
  $write_v4 300
  $total_v1 400
  $feed_v2 250
  $enquire_v2 300
  $complicated_a2 250
  $vital_a2 250
  $urgent_a2 250
  $unsatisfactory_a2 250
  $call_v3 200
  $forbid_v1 250
  $schlep_v1 250
  $edit_n1 400
  $acceptable_a1 200
  $add_v1 120
  $advance_v2 200
  $assure_v1 200
  $available_a2 100
  $Bill 200
  $book_v1 200
  $bring_v3 200
  $call_ditrans_v1 200
  $charge_v3 200
  $consider_v3 200
  $declare_v6 200
  $difficult_a1 200
  $do2 300
  $do_v4 200
  $get_v1 300
  $get_v2 300
  $get_prd_v2 200
  $give_v1 200
  $guy_n1 100
  $have-prd 50
  $important_a1 200
  $impossible_a1 200
  $level_a1 200
  $level_v1 120
  $like_prd_v1 200
  $likely_a1 200
  $make_v3 200
  $necessary_a1 200
;  $need_v1 120
  $need-prd 50
  $want_prd 50
  $offer_v1 200
  $okay_a1 200
  $please_v1 120
  $possible_a1 200
  $prepare_v3 200
  $reasonable_a1 200
  $sell_v1 200
  $shift_v2 120
  $tell_v4 120
  $thanks_n1 100
  $think_v3 220
  $unlikely_a1 200
  $US_n1 100
  $back_n1 150
  $finding_n1 100
  $left_n1 10
  $regard_n1 150
  $right_n1 100
  $crucial_a1 200
  $encouraging_a1 200
  $interesting_a1 200
  $optional_a1 200
  $profitable_a1 200
  $unnecessary_a1 200
  $cost_v1 200
  $in_a1 200
  $need_n1 50
  $through_a1 200
  $thru_a1 200
  $on_a1 200
  $used_a2 200
  $will_n1 100
  $number_v1 120
  $mail_v2 120
;  $return_v5 200
  $electric_n1 10
  $attempt_v1 120
  $leave_n1 10
  $resend_v2 200
  $ship_v3 200
;  $order_n2 10
  $cloud_v1 120
  $synchronize_v3 120
  $so_a1 200
  $wonder_v3 120
  $crown_v2 200
  $freeze_v1 120
  $pocket_v1 120
  $hold_up_n1 100
  $complicated_a1 200
  $vital_a1 200
  $urgent_a1 200
  $unsatisfactory_a1 200
  $fed_ex_v2 200
  $fed_ex_v2a 200
  $still_not 500
  $following_n1 150
  $profile_n1 150
  $after_pp 200
  $request_v3 300
  $like_p 200
  $no_det 450
  $wish_v4 200
  $number_abb_n1 100
;  $shipping_n1 900
  $before_adv1 300
  $after_pp 300
  $look_seem_v1 200
  $checking_n1 300
  $clear_v1 300
  conj_complex_and_le 900
  conj_complex_and_both_le 900
  conj_complex_and_then_le 900
  conj_complex_then_le 900
  conj_complex_as_well_as_le 900
  conj_complex_as_well_as_both_le 900
  conj_complex_but_le 900
  conj_complex_nor_le 900
  conj_complex_nor_neither_le 900
  conj_complex_or_le 900
  conj_complex_or_either_le 900
  conj_complex_plus_le 900
  conj_complex_so_le 900
  conj_complex_vs_le 900
  conj_complex_minus_le 900
  conj_complex_but_not_le 900
  conj_complex_amp_le 900
  conj_complex_amp_both_le 900
  conj_complex_rather_le 900
  conj_complex_and_or_le 900
  va_quasimodal_le 900
  v_poss_le 400
  n_hour_le 900
  p_ditrans_le 900
  p_ditrans_from_to_le 900
  v_expl_it_subj_like_le 900
  adv_s_pre_word_nospec_le 900
  adj_more_less_le 900
  adj_most_least_le 900
  v_subj_equi_le 850
  v_subj_equi_nonfin_le 850
  n_proper_le 900
;  v_prep_particle_np_le 900
  n_wh_pro_le 900
  n_wh_pro_acc_le 900
  n_rel_pro_what_le 900
  n_rel_pro_who_le 900
  n_rel_pro_acc_le 900
  v_empty_prep_intrans_le 900
;  v_empty_prep_intrans_imp_le 900
  comp_to_prop_le 900
  comp_to_nonprop_le 900
  comp_if_indic_le 900
  conj_and_num_le 900
  adj_complemented_unspecified_card_le 900
  adj_reg_equi_le 900
  v_np_prep_trans_le 900
  v_np_prep_trans_dors_le 900
;  v_np_prep*_trans_le 900
  v_np*_prep_trans_le 900
  p_subconj_le 900
  p_subconj_inf_le 300
;  v_empty_prep*_trans_nosubj_le 900
  v_double_pp_le 900
  adj_trans_le 900
  adj_trans_oblig_le 900
  n_title_le 900
  adj_reg_atrans_cp_le 900
  adj_reg_atrans_le 900
  v_obj_equi_non_trans_prd_imper_le 900
;  v_subj_equi_and_le 900
  det_part_ms_wh_le 900
  det_part_pl_wh_le 900
  v_np_trans_double_pp*_le 900
  v_np*_trans_double_pp*_le 900
  v_np*_trans_double_pp_to*_le 900
  v_expl_it_subj_np_cp_le 900
  v_expl_it_subj_np_cp_fin_le 900
  v_expl_it_subj_np_cp_inf_le 900
  eheader_verb_np_le 800
  n_x_to_y_plur_le 900
  n_mass_count_ppcomp_le 500
  v_empty_to_trans_le 900
  n_generic_pro_adv_le 900
  v_cp_inf_le 900
  v_particle_pp_le 900
  comp_for_le 900
.

;;;
;;; based on selectional dependencies between lexical entries, reduce chart
;;; right after lexical look-up: `chart-dependencies' is a list of pairs of
;;; paths into lexical entries.  the type of the node at the end of the first
;;; path in one lexical entry makes that entry depend on the existence of some
;;; other lexical entry that has that same type as the value of the node at the
;;; end of the second path.
;;;
;;; _fix_me_
;;; not entirely sure, but it must (in principle) be possible to saturate a
;;; dependency from lexical and grammar rules.  say, a lexical entry selected
;;; for something nominalized, and that relation was introduced by a lexical
;;; rule, in turn.  unless this is the case already, compute static list of all
;;; relations introduced by rules (which, presumably, requires another setting
;;; to declare how to find constructional semantic contributions; C-CONT) and
;;; consider all such dependencies on lexical entries always saturated.
;;;                                                          (11-oct-02; oe)
unidirectional-chart-dependencies.
chart-dependencies := 
;  "SYNSEM.LOCAL.CAT.VAL.SUBJ.FIRST.LOCAL.CONT.INDEX"
;  "SYNSEM.LOCAL.CONT.INDEX"
  "SYNSEM.LKEYS.--+COMPKEY" "SYNSEM.LOCAL.KEYS.KEY"
  "SYNSEM.LKEYS.--+OCOMPKEY" "SYNSEM.LOCAL.KEYS.KEY"
.
