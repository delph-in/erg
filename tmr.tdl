;;; -*- Mode: tdl; Coding: utf-8; -*-

;;;
;;; token mapping is the process of inspecting and re-arranging input tokens,
;;; i.e. a lattice of structured objects (feature structures), to best match
;;; the expectations of the grammar proper.  the general mechnism is described
;;; by Adolphs, et al. (2008); see:
;;;
;;;   http://www.lrec-conf.org/proceedings/lrec2008/summaries/349.html
;;;
;;; as of August 2008, we are assuming an initial tokenization that is (mostly)
;;; compatible to Penn Treebank (PTB) conventions; this may or may not turn out
;;; to be a good idea, but if nothing else it makes the core parser compatible
;;; with a wide variety of existing tools and pre-processing approaches.  for a
;;; critical (and subjective) discussion of PTB tokenization issues, see:
;;;
;;;  http://lingpipe-blog.com/2008/06/26/the-curse-of-intelligent-tokenization/
;;;
;;; in the process of token mapping, we move from a PTB-compatible tokenization
;;; to an ERG-compatible one: specifically, many punctuation marks are attached
;;; as prefixes or suffixes on other tokens.  the process is broken down into a
;;; number of (more or less) distinct phases, viz.
;;;
;;; - normalization: anything the (ideal) tokenizer _should_ have done.
;;; - NE recognition: surface-based identification of URLs, numbers, et al.
;;; - tokenization adjustments: hyphens, slashes, sandwiched punctuation.
;;; - decoration: filling in missing or underspecified token properties.
;;; - token combination: re-attach punctuation marks and contracted forms.
;;; - PoS explosion: multiply out alternate PoS readings as separate tokens.
;;; - PoS reduction: prune low-probability, unwanted, and overlapping tags.
;;;
;;; we hope we have (now) arrived at a relatively stable inventory of token
;;; properties, of which some typically are only introduced in token mapping;
;;; these are ONSET, PRED, CARG.  however, in principle a tokenizer might pass
;;; in any of these properties, or they could be introduced very early in the
;;; rewrite process.  hence, all rules must make sure to always preserve all
;;; token information.
;;;

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;
;;; correct tokenization `damage', inherited from the PTB conventions.
;;;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

;;
;; convert (PTB-style) ASCII directional quotes to UniCode characters.  when
;; running from an actual PTB file, these are what we would see as input.
;;
opening_double_quote := one_one_form_tmt &
[ +INPUT < [ +FORM "``" ] >,
  +OUTPUT < [ +FORM "“" ] > ].
           
closing_double_quote := one_one_form_tmt &
[ +INPUT < [ +FORM "''" ] >,
  +OUTPUT < [ +FORM "”" ] > ].
           
;;
;; {|do| |has| |wo| ...} |n't| --> {|don't| |hasn't| |won't| 
;;
contracted_negation_tmr := two_one_initial_form_tmt &
[ +INPUT < [ +FORM "/([[:alpha:]]+)/" ],
           [ +FORM "/(n't|N'T)/" ] >,
  +OUTPUT < [ +FORM "${I1:+FORM:1}${I2:+FORM:1}" ] > ].


;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;
;;; spell correction: a few high-frequency typos, not introducing ambiguity.
;;; in principle, we should maybe also have confusion pairs (|their|, |there|),
;;; as ambiguity-introducing rules.  but then we would need a way of turning
;;; on this latter class selectively, i.e. when parsing carefully edited text,
;;; these rules would (at best) introduce spurious ambiguity.
;;;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

sc_didnt_tmr := one_one_form_tmt &
[ +INPUT < [ +FORM "/([dD])idn;?t/"] >,
  +OUTPUT < [ +FORM "${I1:+FORM:1}didn't" ] > ].

sc_dont_tmr := one_one_form_tmt &
[ +INPUT < [ +FORM "/([dD])ont/"] >,
  +OUTPUT < [ +FORM "${I1:+FORM:1}on't" ] > ].

sc_everytime_tmr := one_two_all_form_tmt &
[ +INPUT < [ +FORM "/[eE]verytime/"] >,
  +OUTPUT < [ +FORM "${I1:+FORM:1}very" ], [ +FORM "time" ] > ].

;;
;; _fix_me_
;; this one, in principle, conflicts with `many-syllabled' and `syllabling'.
;; maybe these would be better addressed as alternate, robust lexical entries?
;;                                                              (23-sep-08; oe)
sc_lable_tmr := one_one_form_tmt &
[ +INPUT < [ +FORM "/([lL])abl(ed|ing)?/"] >,
  +OUTPUT < [ +FORM "${I1:+FORM:1}abel${I1:+FORM:2}" ] > ].

sc_recieve_tmr := one_one_form_tmt &
[ +INPUT < [ +FORM "/([rR])eciev(e|ed|es|ing)/"] >,
  +OUTPUT < [ +FORM "${I1:+FORM:1}eceiv${I1:+FORM:2}" ] > ].

sc_wont_tmr := one_one_form_tmt &
[ +INPUT < [ +FORM "/([wW])ont/"] >,
  +OUTPUT < [ +FORM "${I1:+FORM:1}on't" ] > ].


;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;
;;; lightweight NEs: form-driven generic entries (formerly `ersatz' entries)
;;;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

;;;
;;; email addresses
;;;

;;
;; any valid DNS string, prefixed by address, with optional angle brackets
;;
email_ne_tmr := ne_tmt &
[ +INPUT < [ +FORM
             "/<?[[:alnum:]._-]+@[[:alnum:]_-]+(\\.[[:alnum:]_-]+)+>?/" ] >,
  +OUTPUT < [ +CLASS email_ne ] > ].

;;;
;;; uniform resource locators (URLs)
;;;

;;
;; any valid DNS string, prefixed by `http://', with optional angle brackets
;;
url_ne_1_tmr := ne_tmt &
[ +INPUT < [ +FORM "/<?http://[[:alnum:]_-]+(\\.[[:alnum:]_-]+)+>?/" ] >,
  +OUTPUT < [ +CLASS url_ne ] > ].

;;
;; any valid DNS string, prefixed by `www', with optional angle brackets
;;
url_ne_2_tmr := ne_tmt &
[ +INPUT < [ +FORM "/<?www(\\.[[:alnum:]_-]+)+>?/" ] >,
  +OUTPUT < [ +CLASS url_ne ] > ].

;;
;; any valid DNS string, with obligatory angle brackets
;;
url_ne_3_tmr := ne_tmt &
[ +INPUT < [ +FORM "/<[[:alnum:]_-]+(\\.[[:alnum:]_-]+)+>/" ] >,
  +OUTPUT < [ +CLASS url_ne ] > ].

;;;
;;; file names
;;;

;;
;; fully-qualified Un*x style, starting with a slash, e.g. |/etc/config|.  
;;
;; _fix_me_
;; we require a minimum of two components, such that |/etc| by itself will not
;; match.  maybe we should allow these too but create an ambiguity here, i.e.
;; output two tokens, one [ CLASS file_ne }, the other [ CLASS non_ne ]?
;;                                                              (19-sep-08; oe)
;;
file_ne_tmr := ne_tmt &
[ +INPUT < [ +FORM "/(/[[:alnum:]._-]+){2,}/?/" ] >,
  +OUTPUT < [ +CLASS file_ne ] > ].


;;;
;;; time-of-day expressions: |9am|, |11:45pm|, |20:15|
;;;

;;
;; an |am| or |pm| suffix unambiguously indicates a time expression.  we also
;; grab all tokens of the form `H:M' where `H' and `M' are numbers in the right
;; ranges.
;;
;; _fix_me_
;; i wonder about `mix in a ratio of 1:15', which the second rule below would
;; consider a time-of-day expression.  should we approach those case with more
;; `optional' NE rules, i.e. ones outputting two tokens?  or should we rather
;; introduce an abstraction over `time_ne' and `ratio_ne', such that a single
;; token can activate multiple lexical entries?  once we get regular expression
;; matching for lexical instantiation (peter is working on that), in principle,
;; we could just drop `time_ne_2_tmr', make `time_ne_ a sub-type of `ratio_ne',
;; and put the `H:M' regular expression into the generic lexical entry.  with
;; great power comes great responsibility :-).                 (19-sep-08; oe)
;;                       
time_ne_1_tmr := ne_tmt &
[ +INPUT < [ +FORM "/(0?[0-9]|1[0-2])(:[0-5][0-9])?([aApP][mM])/" ] >,
  +OUTPUT  < [ +CLASS time_ne ] > ].

time_ne_2_tmr := ne_tmt &
[ +INPUT < [ +FORM "/(0?[0-9]|1[0-9]|2[0-4]):[0-5][0-9]/" ] >,
  +OUTPUT  < [ +CLASS time_ne ] > ].

;;;
;;; ratios: |1:1000|, |1:100,000|, et al.
;;;

;;
;; we make the conservative assumption that the first element not exceed three
;; digits and not have leading zeros.
;;
ratio_ne_1_tmr := ne_tmt &
[ +INPUT < [ +FORM "/[1-9][0-9]{0,2}:[1-9][0-9]*/" ] >,
  +OUTPUT < [ +CLASS meas_ne ] > ].   

ratio_ne_2_tmr := ne_tmt &
[ +INPUT < [ +FORM "/[1-9][0-9]{0,2}:[1-9][0-9]{2}(,[0-9]{1,3})*/" ] >,
  +OUTPUT < [ +CLASS meas_ne ] > ].   

;;;
;;; numerals, including some sub-sets (days of the month or years).
;;;

;;
;; days of the month: |1| -- |9|, |10| -- |29|, |30|, and |31|
;;
card_or_dom_ne_tmr := ne_tmt &
[ +INPUT < [ +FORM "/([1-9]|[1-2][0-9]|3[01])/" ] >,
  +OUTPUT < [ +CLASS card_or_dom_ne ] > ].

;;
;; (candidate) years: |950|, |1805|, |1957|, |2005|, et al.
;;
;; _fix_me_
;; i would like to split off the |mid-| prefix; i guess it can attach to all
;; sorts of nouns and has pretty regular semantics too.         (21-sep-08; oe)
;;
card_or_year_ne_tmr := ne_tmt &
[ +INPUT < [ +FORM "/(mid-)?[12]?[0-9]{3}/" ] >,
  +OUTPUT < [ +CLASS card_or_year_ne ] > ].

;;
;; any sequence of digits, with optional sign and optional decimal point.
;;
card_ne_1_tmr := ne_tmt &
[ +INPUT < [ +FORM "/[+-]?[1-9][0-9]*\\.?/" ] >,
  +OUTPUT < [ +CLASS card_ne ] > ].

;;
;; floating point numbers, with optional sign and at least one decimal
;;
card_ne_2_tmr := ne_tmt &
[ +INPUT < [ +FORM "/[+-]?([1-9][0-9]*)?\\.[0-9]+/" ] >,
  +OUTPUT < [ +CLASS card_ne ] > ].

;;
;; US-style or German separators, optional sign and decimals: e.g. |23,000.-|
;;
card_ne_3_tmr := ne_tmt &
[ +INPUT < [ +FORM "/[+-]?[1-9][0-9]{2}([,.][0-9]{3})+([,.]([0-9]*|-))?/" ] >,
  +OUTPUT < [ +CLASS card_ne ] > ].


;;;
;;; various kinds of identifiers (accumulated throughout the years); some of
;;; these are allowed to include punctuation marks that can (later) lead to
;;; additional token boundaries (i.e. |-| and |/|), hence we need to be fairly
;;; restrictive about our identifier patters.  in principle, i guess, this is
;;; something that should be adapted specifically for a target domain and type
;;; of text.
;;;

;;
;; _fix_me_
;; a somewhat brave generalization: what seems common to the (relatively) many
;; patterns for identifiers in the ERG is the combination of alphabetic and
;; numeric characters in a single token.  where else (than in names), would we
;; expect that?  to test our hypothesis, treat all such tokens as identifiers.
;; plus a few others, of course, that also can include punctuation marks.
;;                                                             (23-sep-08; oe)
alphanumeric_identifier_ne_1_tmr := ne_tmt &
[ +INPUT < [ +FORM "/([0-9]+[[:alpha:]]+)+/" ] >,
  +OUTPUT < [ +CLASS proper_ne ] > ].

;;
;; a special case: one hyphen, followed by a single letter: |22-b|
;;
alphanumeric_identifier_ne_2_tmr := ne_tmt &
[ +INPUT < [ +FORM "/[0-9]+-[[:alpha:]]/" ] >,
  +OUTPUT < [ +CLASS proper_ne ] > ].

;;
;; in a similar spirit, a number followed by letters in paratheses: |22(B)|;
;; note that the closing parenthese will have been tokenized off, PTB-style.
;;
alphanumeric_identifier_ne_3_tmr := two_one_tmt &
[ +INPUT < [ +FORM "/([0-9]+\\([[:alpha:]]+[[:alnum:]]*)/", 
             +ONSET #onset, +CLASS non_ne,
             +PRED #pred, +CARG #carg ],
           [ +FORM ")", +CLASS non_ne ] >,
  +OUTPUT < [ +FORM "${I1:+FORM:1})", 
              +ONSET #onset, +CLASS proper_ne,
              +PRED #pred, +CARG #carg, +TNT null_tnt ] > ].

;;
;; strictly alphanumeric strings (i.e. including digits), with at least two
;; hyphens, e.g. |123-45-6789|.
;;
hyphenated_identifier_ne_tmr := ne_tmt &
[ +INPUT < [ +FORM 
             "/[[:alpha:]]*[0-9]+[[:alpha:]0-9]*(-[[:alnum:]]+){2,}/" ] >,
  +OUTPUT < [ +CLASS proper_ne ] > ].

;;
;; the following maybe are taken from chemistry: |B.25| |IL-10| |IL/10|
;;
chemistry_identifier_ne_tmr := ne_tmt &
[ +INPUT < [ +FORM "/[0-9]*[[:upper:]]+[-./][[:upper:]]*[0-9]+/" ] >,
  +OUTPUT < [ +CLASS proper_ne ] > ].

;;
;; section numbers (and the like): two or more decimal points
;;
section_number_ne_tmr := ne_tmt &
[ +INPUT < [ +FORM "/[0-9]+\\.[0-9]+(\\.[0-9]+/)+" ] >,
  +OUTPUT < [ +CLASS proper_ne ] > ].


;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;
;;; now with NEs out of our way, this would be a good time for adjustments to
;;; tokenization: introduce additional token boundaries (e.g. for hyphens and 
;;; slashes) and maybe some robustness rules for `sandwiched' punctuation.
;;;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;


;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;
;;; some tokenizers (e.g. the one of acrolinx) already distinguish a number of
;;; token classes.  our REPP tokenizer, however, does not; so, determine class
;;; values here, if need be.  with acrolinx, we might have to map their naming
;;; scheme into our type hierarchy, on the other hand.
;;;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

numeric_class_tmr := token_class_tmt &
[ +INPUT < [ +FORM "/[[:digit:]]+/" ] >,
  +OUTPUT < [ +CLASS numeric ] > ].

alphabetic_class_tmr := token_class_tmt &
[ +INPUT < [ +FORM "/[[:alpha:]]+/" ] >,
  +OUTPUT < [ +CLASS alphabetic ] > ].

alphanumeric_class_tmr := token_class_tmt &
[ +INPUT < [ +FORM "/[[:alnum:]_-]+/" ] >,
  +OUTPUT < [ +CLASS alphanumeric ] > ].

non_alphanumeric_class_tmr := token_class_tmt &
[ +OUTPUT < [ +CLASS non_alphanumeric ] > ].

;;
;; further decorate the token class with information about (a) sentence-initial
;; position and (b) capitalization.  because these are attributes of +CLASS
;; (and there is no way of overwriting), we play a nasty trick on +CARG, viz.
;; utilize it as a `scratch' slot to prevent cyclic rule applications.  we
;; (kind of) assume that no external tokenizer will pass in +CARG values; and
;; if it did, the worst that would happen is that the rules below cannot fire.
;;
non_initial_tmr := token_case_tmt &
[ +CONTEXT < [] >,
  +INPUT < [ +CARG anti_string ] >,
  +OUTPUT < [ +CLASS [ +INITIAL - ], +CARG non_string ] >,
  +POSITION "C1<I1" ].

initial_tmr := token_case_tmt &
[ +INPUT < [ +CARG anti_string ] >,
  +OUTPUT < [ +CLASS #class & [ +INITIAL + ], +CARG non_string ] > ].

lower_tmr := token_case_tmt &
[ +INPUT < [ +FORM "/[[:lower:][:digit:]_-]+/", 
             +CARG non_string ] >,
  +OUTPUT < [ +CLASS [ +CASE lower ], 
              +CARG anti_string ] > ].

upper_tmr := token_case_tmt &
[ +INPUT < [ +FORM "/[[:upper:][:digit:]_-]+/", 
             +CARG non_string ] >,
  +OUTPUT < [ +CLASS [ +CASE upper ], 
              +CARG anti_string ] > ].
  
capitalized_tmr := token_case_tmt &
[ +INPUT < [ +FORM "/[[:upper:]][[:alnum:]_-]+/", 
             +CARG non_string ] >,
  +OUTPUT < [ +CLASS [ +CASE capitalized+mixed ],
              +CARG anti_string ] > ].

non_capitalized_tmr := token_case_tmt &
[ +INPUT < [ +FORM "/[[:lower:][:digit:]][[:alnum:]_-]+/", 
             +CARG non_string ] >,
  +OUTPUT < [ +CLASS [ +CASE non_capitalized+mixed ],
              +CARG anti_string ] > ].

;;
;; in case we are running without a PoS tagger, or something went wrong in the
;; creation of token AVMs from our input (in one form or another), make sure to
;; fully annul part-of-speech information.
;;
null_tnt_tmr := token_mapping_rule &
[ +INPUT < [ +FORM #form, +ONSET #onset, +CLASS #class,
             +PRED #pred, +CARG #carg, +ID #id, +FROM #from, +TO #to,
             +TNT [ +TAGS < anti_string, ... > ] ] >,
  +OUTPUT < [ +FORM #form, +ONSET #onset, +CLASS #class,
              +PRED #pred, +CARG #carg, +ID #id, +FROM #from, +TO #to, 
              +TNT null_tnt ] > ].


;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;
;;; re-combine punctuation marks with adjacent tokens, based on directionality
;;; of punctuation marks, e.g. opening vs. closing quotes and brackets.  doing
;;; one such re-combination at a time is sufficient, as each rewrite rule will
;;; apply as many times as it possible can, seeing its own output from earlier
;;; applications.
;;;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

;;
;; but first, preserve the current (non-punctuated) from in +CARG, for later
;; reference, e.g. in constructing +PRED values for generics.  NE rules have
;; done this already, hence make sure to not overwrite existing +CARGSs.
;;
default_carg_tmr := token_mapping_rule &
[ +INPUT < [ +FORM #form, +ONSET #onset, +CLASS #class,
             +PRED #pred, +CARG anti_string,
             +ID #id, +FROM #from, +TO #to, +TNT #tnt ] >,
  +OUTPUT < [ +FORM #form, +ONSET #onset, +CLASS #class,
              +PRED #pred, +CARG #form,
              +ID #id, +FROM #from, +TO #to, +TNT #tnt ] > ].

prefix_punctuation_tmr := two_one_final_form_tmt &
[ +INPUT < [ +FORM "/([[({“‘]+)/" ], 
           [ +FORM "/(.+)/" ] >,
  +OUTPUT < [ +FORM "${I1:+FORM:1}${I2:+FORM:1}" ] > ].

;;
;; _fix_me_
;; there is a special case here: |'| following a token ending in |s| could be a
;; possessive marker (which should remain a token in its own right), or could 
;; be a closing single quote.  in principle, the same is true for |"|, but the
;; `inches' measure unit, maybe, will have been detected during NE recognition
;; earlier.  in either case, we would need a way of keeping a separate |'| in
;; the chart, and also re-combine it with the preceding token.  (14-sep-08; oe)
;;
suffix_punctuation_tmr := two_one_initial_form_tmt &
[ +INPUT < [ +FORM "/(.+)/" ],
           [ +FORM "/([])}”\"’,;.!?]+)/" ] >,
  +OUTPUT < [ +FORM "${I1:+FORM:1}${I2:+FORM:1}" ] > ].


;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;
;;; at this point, we multiply out PoS values on all tokens, where for each
;;; original token as many additional tokens are created (in the same chart
;;; cell) as there are PoS readings.  at this point, we start distinguishing
;;; between tokens that activate native lexical entries (LEs), vs. those that
;;; activate generic LEs.  in the token universe, this distinction is made by
;;; virtue of +ONSET, with unk_onset reserved for generic LEs.  the two sets
;;; do not overlap, i.e. for a single original token with two PoS readings, we
;;; end up with a total of three new tokens.  the pair of rules below resembles
;;; a recursive function, terminating once the PoS list has been reduced to 
;;; a singleton element.  form-based named entities identified earlier avoid
;;; this kind of PoS multiplication because they have already emptied out their
;;; PoS list.
;;;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

tnt_recurse_rule := token_mapping_rule &
[ +INPUT < [ +FORM #form, +ONSET anti_onset,
             +CLASS #class, +PRED #pred, +CARG #carg,
             +ID #id, +FROM #from, +TO #to,
             +TNT [ +TAGS < #tag . #tags & *cons* >,
                    +PRBS < #prb . #prbs & *cons* > ] ] > ,
  +OUTPUT < [ +FORM #form, +ONSET unk_onset,
              +CLASS #class, +PRED #pred, +CARG #carg,
              +ID #id, +FROM #from, +TO #to,
              +TNT [ +TAGS < #tag >, +PRBS < #prb > ] ],
            [ +FORM #form,
              +CLASS #class, +PRED #pred, +CARG #carg,
              +ID #id, +FROM #from, +TO #to,
              +TNT [ +TAGS #tags, +PRBS #prbs ] ] > ,
  +POSITION "O1=O2" ].

tnt_terminate_tmr := token_mapping_rule &
[ +INPUT < [ +FORM #form, +ONSET anti_onset,
             +CLASS #class, +PRED #pred, +CARG #carg,
             +ID #id, +FROM #from, +TO #to,
             +TNT [ +TAGS < #tag >, +PRBS < #prb > ] ] > ,
  +OUTPUT < [ +FORM #form, +ONSET unk_onset,
              +CLASS #class, +PRED #pred, +CARG #carg,
              +ID #id, +FROM #from, +TO #to,
              +TNT [ +TAGS < #tag >, +PRBS < #prb > ] ],
            [ +FORM #form, +ONSET con_or_voc,
              +CLASS #class, +PRED #pred, +CARG #carg,
              +ID #id, +FROM #from, +TO #to,
              +TNT null_tnt ] >,
  +POSITION "O1=O2" ].

;;;
;;; with singleton PoS readings multiplied out in each chart cell, we can prune
;;; undesirable alternatives, e.g. a foreign word reading when there also is a
;;; common noun.  also, ditch PoS readings with very low probability, and ones
;;; for which no PoS-activated generic entries exist anyway.
;;;
;;; _fix_me_
;;; should we eventually want to include the PoS probabilities as a feature in
;;; parse selection, this kind of pruning should disappear: a high-probability
;;; FW, say, should not be bullied out by an unlike NN.         (31-aug-08; oe)
;;;

tnt_ditch_unlikely_tmr := token_mapping_rule &
[ +INPUT < [ +TNT.+PRBS < "/0?\\.0.*/" > ] >,
  +OUTPUT < > ].

tnt_ditch_function_tmr := token_mapping_rule &
[ +INPUT < [ +TNT.+TAGS 
             < "/CC|DT|EX|IN|MD|PDT|POS|PRP\\$?|RP|TO|UH|WDT|WP|WRB/" > ] >,
  +OUTPUT < > ].

tnt_ditch_punctuation_tmr := token_mapping_rule &
[ +INPUT < [ +TNT.+TAGS < "/\\$|#|``|''|\\(|\\)|,|\\.|:/" > ] >,
  +OUTPUT < > ].

tnt_filter_dup_fw_tmr := token_mapping_rule &
[ +CONTEXT  < [ +TNT.+TAGS < "NN" > ] >,
  +INPUT    < [ +TNT.+TAGS < "FW" > ] >,
  +OUTPUT   < >,
  +POSITION "I1=C1" ].

tnt_filter_dup_nnp_tmr := token_mapping_rule &
[ +CONTEXT  < [ +TNT.+TAGS < "/FW|NN/" > ] >,
  +INPUT    < [ +TNT.+TAGS < "NNP" > ] >,
  +OUTPUT   < >,
  +POSITION "I1=C1" ].

tnt_filter_dup_nnps_tmr := token_mapping_rule &
[ +CONTEXT  < [ +TNT.+TAGS < "NNP" > ] >,
  +INPUT    < [ +TNT.+TAGS < "NNPS" > ] >,
  +OUTPUT   < >,
  +POSITION "I1=C1" ].

;;
;; _fix_me_
;; the following seems to not work, in that ${lc(I1:+TNT.+TAGS.+FIRST:1)} ends
;; up as a literal in the result; ditching the `lc(...) makes no difference
;; either, so i suspect there may be a bug related to the `+TNT.+TAGS.+FIRST'
;; path.  check with peter.                                    (19-sep-08; oe)
;;
#|
generic_pred_tmr := token_mapping_rule &  
[ +INPUT < [ +FORM #form & "/(.+)/",
             +ONSET #onset & unk_onset, +CLASS #class & non_ne, 
             +PRED anti_string, +CARG #carg,
             +ID #id, +FROM #from, +TO #to,
             +TNT #tnt & [ +TAGS < "/(.+)/" > ] ] >,
  +OUTPUT < [ +FORM #form,
              +ONSET #onset, +CLASS #class,
              +PRED "_${I1:+FORM:1}_${lc(I1:+TNT.+TAGS.+FIRST:1)}_rel",
              +CARG #carg,
              +ID #id, +FROM #from, +TO #to,
              +TNT #tnt ] > ].

|#

;;
;; _fix_me_
;; but neither does this version work.  it fails in lexical instantiation (the
;; `generic_mass_count_noun' entry is the one we want), claiming:
;;
;;   type clash under FIRST.+PRED: `predsort' & `"_fooz_n_rel"'
;;
;; but `string' is a sub-type of `predsort', so these should unify!  almost as
;; if the strings resulting from RE captures are weird in some way?
;;                                                             (21-sep-08; oe)
;;
#|
generic_pred_nn_tmr := token_mapping_rule &  
[ +INPUT < [ +FORM #form,
             +ONSET #onset & unk_onset, +CLASS #class & non_ne, 
             +PRED anti_string, +CARG #carg & "/(.+)/",
             +ID #id, +FROM #from, +TO #to,
             +TNT #tnt & [ +TAGS < "/NNS?/" > ] ] >,
  +OUTPUT < [ +FORM #form,
              +ONSET #onset, +CLASS #class, 
              +PRED "_${I1:+CARG:1}_n_rel", +CARG #carg,
              +ID #id, +FROM #from, +TO #to, +TNT #tnt ] > ].
|#

generic_pred_nn_tmr := token_mapping_rule &  
[ +INPUT < [ +FORM #form,
             +ONSET #onset & unk_onset, +CLASS #class & non_ne, 
             +PRED anti_string, +CARG #carg,
             +ID #id, +FROM #from, +TO #to,
             +TNT #tnt & [ +TAGS < "/NNS?/" > ] ] >,
  +OUTPUT < [ +FORM #form,
              +ONSET #onset, +CLASS #class, 
              +PRED "unknown_n_rel", +CARG #carg,
              +ID #id, +FROM #from, +TO #to, +TNT #tnt ] > ].

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;
;;; finally, make all non-generic tokens lower case, for lexical look-up.
;;;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

downcase_tmr := one_one_form_tmt &
[ +INPUT < [ +FORM "/(.*[[:upper:]].*)/", +ONSET con_or_voc ] > ,
  +OUTPUT < [ +FORM "${lc(I1:+FORM:1)}" ] > ].

